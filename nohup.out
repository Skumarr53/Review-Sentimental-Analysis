ERROR: You need to initialize the database. Please run `airflow db init`. Make sure the command is run using Airflow version 2.9.1.
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T15:09:14.207+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T15:09:14.208+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 15:09:14 +0530] [59948] [INFO] Starting gunicorn 22.0.0
[2024-05-18 15:09:14 +0530] [59948] [INFO] Listening at: http://[::]:8793 (59948)
[2024-05-18 15:09:14 +0530] [59948] [INFO] Using worker: sync
[[34m2024-05-18T15:09:14.239+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T15:09:14.240+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 15:09:14 +0530] [59949] [INFO] Booting worker with pid: 59949
[[34m2024-05-18T15:09:14.245+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 59950[0m
[[34m2024-05-18T15:09:14.247+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:09:14.249+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T15:09:14.272+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 15:09:14 +0530] [59951] [INFO] Booting worker with pid: 59951
[2024-05-18 15:09:18 +0530] [59948] [INFO] Handling signal: winch
[2024-05-18 15:11:53 +0530] [59948] [INFO] Handling signal: int
[[34m2024-05-18T15:11:53.393+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 15:11:53 +0530] [59949] [INFO] Worker exiting (pid: 59949)
[2024-05-18 15:11:53 +0530] [59951] [INFO] Worker exiting (pid: 59951)
[2024-05-18 15:11:53 +0530] [59948] [INFO] Shutting down: Master
[[34m2024-05-18T15:11:54.407+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 59950. PIDs of all processes in the group: [59950][0m
[[34m2024-05-18T15:11:54.407+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 59950[0m
[[34m2024-05-18T15:11:54.579+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=59950, status='terminated', exitcode=0, started='15:09:14') (59950) terminated with exit code 0[0m
[[34m2024-05-18T15:11:54.589+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 59950. PIDs of all processes in the group: [][0m
[[34m2024-05-18T15:11:54.590+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 59950[0m
[[34m2024-05-18T15:11:54.590+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 59950 as process group is missing.[0m
[[34m2024-05-18T15:11:54.590+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T15:12:22.828+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T15:12:22.829+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 15:12:22 +0530] [60571] [INFO] Starting gunicorn 22.0.0
[2024-05-18 15:12:22 +0530] [60571] [INFO] Listening at: http://[::]:8793 (60571)
[2024-05-18 15:12:22 +0530] [60571] [INFO] Using worker: sync
[[34m2024-05-18T15:12:22.860+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T15:12:22.860+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 15:12:22 +0530] [60572] [INFO] Booting worker with pid: 60572
[[34m2024-05-18T15:12:22.866+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 60574[0m
[2024-05-18 15:12:22 +0530] [60573] [INFO] Booting worker with pid: 60573
[[34m2024-05-18T15:12:22.867+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:12:22.870+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T15:12:22.892+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 15:14:41 +0530] [60571] [INFO] Handling signal: int
[[34m2024-05-18T15:14:41.904+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 15:14:42 +0530] [60572] [INFO] Worker exiting (pid: 60572)
[2024-05-18 15:14:42 +0530] [60573] [INFO] Worker exiting (pid: 60573)
[2024-05-18 15:14:42 +0530] [60571] [INFO] Shutting down: Master
[[34m2024-05-18T15:14:42.918+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 60574. PIDs of all processes in the group: [60574][0m
[[34m2024-05-18T15:14:42.918+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 60574[0m
[[34m2024-05-18T15:14:43.091+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=60574, status='terminated', exitcode=0, started='15:12:22') (60574) terminated with exit code 0[0m
[[34m2024-05-18T15:14:43.109+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 60574. PIDs of all processes in the group: [][0m
[[34m2024-05-18T15:14:43.109+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 60574[0m
[[34m2024-05-18T15:14:43.109+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 60574 as process group is missing.[0m
[[34m2024-05-18T15:14:43.109+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T15:18:00.634+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T15:18:00.634+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 15:18:00 +0530] [62038] [INFO] Starting gunicorn 22.0.0
[2024-05-18 15:18:00 +0530] [62038] [INFO] Listening at: http://[::]:8793 (62038)
[2024-05-18 15:18:00 +0530] [62038] [INFO] Using worker: sync
[[34m2024-05-18T15:18:00.665+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T15:18:00.665+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 15:18:00 +0530] [62039] [INFO] Booting worker with pid: 62039
[[34m2024-05-18T15:18:00.671+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 62040[0m
[[34m2024-05-18T15:18:00.672+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:18:00.676+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T15:18:00.699+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 15:18:00 +0530] [62041] [INFO] Booting worker with pid: 62041
[[34m2024-05-18T15:18:41.901+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 15:18:41 +0530] [62038] [INFO] Handling signal: int
[2024-05-18 15:18:42 +0530] [62039] [INFO] Worker exiting (pid: 62039)
[2024-05-18 15:18:42 +0530] [62041] [INFO] Worker exiting (pid: 62041)
[2024-05-18 15:18:42 +0530] [62038] [INFO] Shutting down: Master
[[34m2024-05-18T15:18:42.911+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 62040. PIDs of all processes in the group: [62040][0m
[[34m2024-05-18T15:18:42.911+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 62040[0m
[[34m2024-05-18T15:18:43.083+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=62040, status='terminated', exitcode=0, started='15:18:00') (62040) terminated with exit code 0[0m
[[34m2024-05-18T15:18:43.093+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 62040. PIDs of all processes in the group: [][0m
[[34m2024-05-18T15:18:43.093+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 62040[0m
[[34m2024-05-18T15:18:43.093+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 62040 as process group is missing.[0m
[[34m2024-05-18T15:18:43.093+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T15:23:17.172+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T15:23:17.173+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 15:23:17 +0530] [62901] [INFO] Starting gunicorn 22.0.0
[2024-05-18 15:23:17 +0530] [62901] [INFO] Listening at: http://[::]:8793 (62901)
[2024-05-18 15:23:17 +0530] [62901] [INFO] Using worker: sync
[[34m2024-05-18T15:23:17.201+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T15:23:17.202+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 15:23:17 +0530] [62903] [INFO] Booting worker with pid: 62903
[[34m2024-05-18T15:23:17.209+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 62904[0m
[[34m2024-05-18T15:23:17.210+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:23:17.212+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T15:23:17.236+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 15:23:17 +0530] [62905] [INFO] Booting worker with pid: 62905
[[34m2024-05-18T15:26:30.468+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 15:26:30 +0530] [62901] [INFO] Handling signal: int
[2024-05-18 15:26:30 +0530] [62905] [INFO] Worker exiting (pid: 62905)
[2024-05-18 15:26:30 +0530] [62903] [INFO] Worker exiting (pid: 62903)
[2024-05-18 15:26:30 +0530] [62901] [INFO] Shutting down: Master
[[34m2024-05-18T15:26:31.482+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 62904. PIDs of all processes in the group: [62904][0m
[[34m2024-05-18T15:26:31.483+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 62904[0m
[[34m2024-05-18T15:26:31.655+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=62904, status='terminated', exitcode=0, started='15:23:17') (62904) terminated with exit code 0[0m
[[34m2024-05-18T15:26:31.674+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 62904. PIDs of all processes in the group: [][0m
[[34m2024-05-18T15:26:31.675+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 62904[0m
[[34m2024-05-18T15:26:31.675+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 62904 as process group is missing.[0m
[[34m2024-05-18T15:26:31.675+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T15:42:40.306+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T15:42:40.307+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 15:42:40 +0530] [68324] [INFO] Starting gunicorn 22.0.0
[2024-05-18 15:42:40 +0530] [68324] [INFO] Listening at: http://[::]:8793 (68324)
[2024-05-18 15:42:40 +0530] [68324] [INFO] Using worker: sync
[[34m2024-05-18T15:42:40.338+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T15:42:40.339+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 15:42:40 +0530] [68325] [INFO] Booting worker with pid: 68325
[[34m2024-05-18T15:42:40.344+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 68326[0m
[[34m2024-05-18T15:42:40.346+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:42:40.349+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18 15:42:40 +0530] [68327] [INFO] Booting worker with pid: 68327
[2024-05-18T15:42:40.373+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-18T15:43:38.224+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[[34m2024-05-18T15:43:38.295+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:43:38.296+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:43:38.296+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T15:43:38.296+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:43:38.299+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T15:43:38.299+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:43:38.299+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T15:43:38.299+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:43:38.309+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:43:39.305+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:43:42.741+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:43:48.532+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:43:49.525+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:43:52.983+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:13:36.127449+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:43:58.557+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T15:43:58.558+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T15:43:58.565+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:13:36.127449+00:00, map_index=-1, run_start_date=2024-05-18 10:13:53.024557+00:00, run_end_date=2024-05-18 10:13:57.275737+00:00, run_duration=4.25118, state=success, executor_state=success, try_number=1, max_tries=3, job_id=4, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:13:38.297312+00:00, queued_by_job_id=2, pid=69062[0m
[[34m2024-05-18T15:43:58.566+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:13:42.786119+00:00, run_end_date=2024-05-18 10:13:47.330453+00:00, run_duration=4.544334, state=success, executor_state=success, try_number=1, max_tries=3, job_id=3, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:13:38.297312+00:00, queued_by_job_id=2, pid=68929[0m
[[34m2024-05-18T15:43:58.770+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:43:58.770+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:43:58.770+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T15:43:58.771+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:43:58.772+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:43:58.772+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:43:58.773+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:43:58.773+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:43:58.781+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:43:59.771+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:44:03.239+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:44:04.584+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:44:05.582+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:44:09.123+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:44:10.487+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T15:44:10.487+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T15:44:10.490+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:13:36.127449+00:00, map_index=-1, run_start_date=2024-05-18 10:14:09.165898+00:00, run_end_date=2024-05-18 10:14:09.298409+00:00, run_duration=0.132511, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:13:58.771664+00:00, queued_by_job_id=2, pid=69266[0m
[[34m2024-05-18T15:44:10.490+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:14:03.281922+00:00, run_end_date=2024-05-18 10:14:03.414567+00:00, run_duration=0.132645, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=5, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:13:58.771664+00:00, queued_by_job_id=2, pid=69189[0m
[2024-05-18 15:45:15 +0530] [68324] [INFO] Handling signal: winch
[2024-05-18 15:47:29 +0530] [68324] [INFO] Handling signal: winch
[2024-05-18 15:47:30 +0530] [68324] [INFO] Handling signal: winch
[[34m2024-05-18T15:47:40.580+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 15:48:43 +0530] [68324] [INFO] Handling signal: int
[[34m2024-05-18T15:48:43.229+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 15:48:43 +0530] [68325] [INFO] Worker exiting (pid: 68325)
[2024-05-18 15:48:43 +0530] [68327] [INFO] Worker exiting (pid: 68327)
[2024-05-18 15:48:43 +0530] [68324] [INFO] Shutting down: Master
[[34m2024-05-18T15:48:44.241+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 68326. PIDs of all processes in the group: [68326][0m
[[34m2024-05-18T15:48:44.241+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 68326[0m
[[34m2024-05-18T15:48:44.413+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=68326, status='terminated', exitcode=0, started='15:42:40') (68326) terminated with exit code 0[0m
[[34m2024-05-18T15:48:44.429+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 68326. PIDs of all processes in the group: [][0m
[[34m2024-05-18T15:48:44.430+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 68326[0m
[[34m2024-05-18T15:48:44.430+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 68326 as process group is missing.[0m
[[34m2024-05-18T15:48:44.430+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T15:48:47.666+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T15:48:47.667+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 15:48:47 +0530] [72706] [INFO] Starting gunicorn 22.0.0
[2024-05-18 15:48:47 +0530] [72706] [INFO] Listening at: http://[::]:8793 (72706)
[2024-05-18 15:48:47 +0530] [72706] [INFO] Using worker: sync
[[34m2024-05-18T15:48:47.704+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T15:48:47.704+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 15:48:47 +0530] [72707] [INFO] Booting worker with pid: 72707
[[34m2024-05-18T15:48:47.711+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 72708[0m
[[34m2024-05-18T15:48:47.713+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:48:47.715+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18 15:48:47 +0530] [72709] [INFO] Booting worker with pid: 72709
[2024-05-18T15:48:47.741+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-18T15:49:04.531+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T15:49:04.532+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:49:04.532+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T15:49:04.533+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:49:04.533+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:49:04.542+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:49:05.543+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:49:09.066+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:49:10.424+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T15:49:10.431+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:19:09.108005+00:00, run_end_date=2024-05-18 10:19:09.228897+00:00, run_duration=0.120892, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:19:04.532600+00:00, queued_by_job_id=7, pid=72988[0m
[[34m2024-05-18T15:49:10.665+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:49:10.665+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:49:10.666+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:49:10.667+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:49:10.667+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:49:10.676+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:49:11.664+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:49:15.157+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:49:16.599+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T15:49:16.604+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:13:36.127449+00:00, map_index=-1, run_start_date=2024-05-18 10:19:15.201820+00:00, run_end_date=2024-05-18 10:19:15.340412+00:00, run_duration=0.138592, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=9, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:19:10.666656+00:00, queued_by_job_id=7, pid=73093[0m
[2024-05-18 15:49:19 +0530] [72706] [INFO] Handling signal: winch
[2024-05-18 15:49:21 +0530] [72706] [INFO] Handling signal: winch
[2024-05-18 15:49:25 +0530] [72706] [INFO] Handling signal: winch
[[34m2024-05-18T15:50:01.541+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:50:01.541+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:50:01.541+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:50:01.543+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T15:50:01.543+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:50:01.552+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:50:02.584+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:50:06.060+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:20:00.876832+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:50:11.844+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T15:50:11.847+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:20:00.876832+00:00, map_index=-1, run_start_date=2024-05-18 10:20:06.114839+00:00, run_end_date=2024-05-18 10:20:10.623486+00:00, run_duration=4.508647, state=success, executor_state=success, try_number=1, max_tries=3, job_id=10, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:20:01.542332+00:00, queued_by_job_id=7, pid=73741[0m
[[34m2024-05-18T15:50:12.013+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:50:12.013+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:50:12.013+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:50:12.014+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:50:12.014+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:50:12.023+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:50:12.978+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:50:16.483+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:20:00.876832+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:50:25.682+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T15:50:25.686+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:20:00.876832+00:00, map_index=-1, run_start_date=2024-05-18 10:20:16.525725+00:00, run_end_date=2024-05-18 10:20:24.387929+00:00, run_duration=7.862204, state=success, executor_state=success, try_number=1, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:20:12.014073+00:00, queued_by_job_id=7, pid=73784[0m
[[34m2024-05-18T15:50:25.874+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:50:25.874+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:50:25.875+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:50:25.876+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T15:50:25.876+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:50:25.885+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:50:26.872+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:50:30.378+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:51:25.053+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T15:51:25.058+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:20:00.876832+00:00, map_index=-1, run_start_date=2024-05-18 10:20:30.421030+00:00, run_end_date=2024-05-18 10:21:23.819159+00:00, run_duration=53.398129, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:20:25.875625+00:00, queued_by_job_id=7, pid=73983[0m
[[34m2024-05-18T15:51:25.070+0530[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=72708) last sent a heartbeat 59.24 seconds ago! Restarting it[0m
[[34m2024-05-18T15:51:25.083+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 72708. PIDs of all processes in the group: [72708][0m
[[34m2024-05-18T15:51:25.083+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 72708[0m
[[34m2024-05-18T15:51:25.255+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=72708, status='terminated', exitcode=0, started='15:48:47') (72708) terminated with exit code 0[0m
[[34m2024-05-18T15:51:25.262+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 74265[0m
[[34m2024-05-18T15:51:25.267+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T15:51:25.291+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-18T15:53:47.855+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:54:10.335+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T15:54:10.335+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:54:10.335+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T15:54:10.337+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:54:10.337+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:54:10.347+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:54:11.338+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:54:14.796+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:54:16.148+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T15:54:16.153+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:24:14.837579+00:00, run_end_date=2024-05-18 10:24:14.958494+00:00, run_duration=0.120915, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:24:10.336430+00:00, queued_by_job_id=7, pid=74741[0m
[[34m2024-05-18T15:54:16.340+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:54:16.340+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:54:16.340+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:54:16.342+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:54:16.342+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:54:16.350+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:54:17.313+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:54:20.745+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:54:22.077+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T15:54:22.082+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:13:36.127449+00:00, map_index=-1, run_start_date=2024-05-18 10:24:20.785300+00:00, run_end_date=2024-05-18 10:24:20.905380+00:00, run_duration=0.12008, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:24:16.341372+00:00, queued_by_job_id=7, pid=74771[0m
[[34m2024-05-18T15:55:21.858+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:55:21.859+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:55:21.859+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [scheduled]>[0m
[[34m2024-05-18T15:55:21.860+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=4, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:55:21.861+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:55:21.869+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:13:36.127449+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:55:22.844+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:55:26.295+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:13:36.127449+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:55:27.695+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:13:36.127449+00:00', try_number=4, map_index=-1)[0m
[[34m2024-05-18T15:55:27.699+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:13:36.127449+00:00, map_index=-1, run_start_date=2024-05-18 10:25:26.335261+00:00, run_end_date=2024-05-18 10:25:26.458635+00:00, run_duration=0.123374, state=up_for_retry, executor_state=success, try_number=4, max_tries=6, job_id=15, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:25:21.859930+00:00, queued_by_job_id=7, pid=75114[0m
[[34m2024-05-18T15:55:32.985+0530[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun sentimental_analysis_training_pipeline @ 2024-05-18 10:13:36.127449+00:00: manual__2024-05-18T10:13:36.127449+00:00, state:running, queued_at: 2024-05-18 10:13:36.151099+00:00. externally triggered: True> failed[0m
[[34m2024-05-18T15:55:32.985+0530[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=sentimental_analysis_training_pipeline, execution_date=2024-05-18 10:13:36.127449+00:00, run_id=manual__2024-05-18T10:13:36.127449+00:00, run_start_date=2024-05-18 10:13:38.240401+00:00, run_end_date=2024-05-18 10:25:32.985786+00:00, run_duration=714.745385, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-12 00:00:00+00:00, dag_hash=dfae8fa89ec57ec058f50008e44c7e8e[0m
[[34m2024-05-18T15:56:24.752+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:56:24.753+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:56:24.753+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T15:56:24.754+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T15:56:24.755+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:56:24.763+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:56:25.736+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:56:29.199+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:56:34.271+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T15:56:34.275+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:20:00.876832+00:00, map_index=-1, run_start_date=2024-05-18 10:26:29.240186+00:00, run_end_date=2024-05-18 10:26:32.968833+00:00, run_duration=3.728647, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:26:24.753912+00:00, queued_by_job_id=7, pid=75475[0m
[[34m2024-05-18T15:58:48.029+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:59:15.487+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T15:59:15.487+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T15:59:15.487+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T15:59:15.489+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T15:59:15.489+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:59:15.497+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T15:59:16.460+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T15:59:20.026+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T15:59:21.411+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-05-18T15:59:21.414+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:29:20.078798+00:00, run_end_date=2024-05-18 10:29:20.203097+00:00, run_duration=0.124299, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=17, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:29:15.488193+00:00, queued_by_job_id=7, pid=75974[0m
[[34m2024-05-18T15:59:23.035+0530[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun sentimental_analysis_training_pipeline @ 2024-05-05 00:00:00+00:00: scheduled__2024-05-05T00:00:00+00:00, state:running, queued_at: 2024-05-18 10:13:38.212580+00:00. externally triggered: False> failed[0m
[[34m2024-05-18T15:59:23.036+0530[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=sentimental_analysis_training_pipeline, execution_date=2024-05-05 00:00:00+00:00, run_id=scheduled__2024-05-05T00:00:00+00:00, run_start_date=2024-05-18 10:13:38.240027+00:00, run_end_date=2024-05-18 10:29:23.036177+00:00, run_duration=944.79615, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-12 00:00:00+00:00, dag_hash=dfae8fa89ec57ec058f50008e44c7e8e[0m
[[34m2024-05-18T15:59:23.041+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[2024-05-18 15:59:25 +0530] [72706] [INFO] Handling signal: int
[[34m2024-05-18T15:59:25.445+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 15:59:25 +0530] [72709] [INFO] Worker exiting (pid: 72709)
[2024-05-18 15:59:25 +0530] [72707] [INFO] Worker exiting (pid: 72707)
[2024-05-18 15:59:25 +0530] [72706] [INFO] Shutting down: Master
[[34m2024-05-18T15:59:26.459+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 74265. PIDs of all processes in the group: [74265][0m
[[34m2024-05-18T15:59:26.459+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 74265[0m
[[34m2024-05-18T15:59:26.632+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=74265, status='terminated', exitcode=0, started='15:51:25') (74265) terminated with exit code 0[0m
[[34m2024-05-18T15:59:26.643+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 74265. PIDs of all processes in the group: [][0m
[[34m2024-05-18T15:59:26.643+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 74265[0m
[[34m2024-05-18T15:59:26.643+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 74265 as process group is missing.[0m
[[34m2024-05-18T15:59:26.643+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T15:59:51.081+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T15:59:51.082+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 15:59:51 +0530] [76095] [INFO] Starting gunicorn 22.0.0
[2024-05-18 15:59:51 +0530] [76095] [INFO] Listening at: http://[::]:8793 (76095)
[2024-05-18 15:59:51 +0530] [76095] [INFO] Using worker: sync
[[34m2024-05-18T15:59:51.113+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T15:59:51.114+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 15:59:51 +0530] [76096] [INFO] Booting worker with pid: 76096
[[34m2024-05-18T15:59:51.120+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 76097[0m
[[34m2024-05-18T15:59:51.121+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T15:59:51.125+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T15:59:51.148+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 15:59:51 +0530] [76098] [INFO] Booting worker with pid: 76098
[[34m2024-05-18T16:00:52.276+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[[34m2024-05-18T16:00:52.340+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:00:52.341+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:00:52.341+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:00:52.343+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:00:52.343+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:00:52.352+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:00:53.348+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:00:56.829+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:01:02.849+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:01:02.856+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:30:56.872522+00:00, run_end_date=2024-05-18 10:31:01.574974+00:00, run_duration=4.702452, state=success, executor_state=success, try_number=1, max_tries=3, job_id=19, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:30:52.341970+00:00, queued_by_job_id=18, pid=76858[0m
[[34m2024-05-18T16:01:03.080+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:30:58.482647+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:01:03.080+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:01:03.080+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:01:03.081+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 2/16 running and queued tasks[0m
[[34m2024-05-18T16:01:03.081+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:30:58.482647+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:01:03.084+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:01:03.084+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:03.084+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:30:58.482647+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:01:03.085+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:30:58.482647+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:03.085+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:01:03.085+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:03.104+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:04.094+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:01:07.606+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:20:00.876832+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:01:13.639+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:30:58.482647+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:14.600+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:01:18.059+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:30:58.482647+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:01:23.943+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:24.958+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:01:28.543+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:01:37.681+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T16:01:37.681+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:30:58.482647+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:01:37.681+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:01:37.689+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:31:28.600215+00:00, run_end_date=2024-05-18 10:31:36.427512+00:00, run_duration=7.827297, state=success, executor_state=success, try_number=1, max_tries=3, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:31:03.082165+00:00, queued_by_job_id=18, pid=77220[0m
[[34m2024-05-18T16:01:37.690+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:30:58.482647+00:00, map_index=-1, run_start_date=2024-05-18 10:31:18.098889+00:00, run_end_date=2024-05-18 10:31:22.648544+00:00, run_duration=4.549655, state=success, executor_state=success, try_number=1, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:31:03.082165+00:00, queued_by_job_id=18, pid=77136[0m
[[34m2024-05-18T16:01:37.690+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:20:00.876832+00:00, map_index=-1, run_start_date=2024-05-18 10:31:07.647648+00:00, run_end_date=2024-05-18 10:31:12.377856+00:00, run_duration=4.730208, state=success, executor_state=success, try_number=2, max_tries=4, job_id=20, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:31:03.082165+00:00, queued_by_job_id=18, pid=76998[0m
[[34m2024-05-18T16:01:37.896+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:30:58.482647+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:01:37.896+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:01:37.897+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:01:37.897+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 2/16 running and queued tasks[0m
[[34m2024-05-18T16:01:37.897+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:30:58.482647+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:01:37.899+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:01:37.899+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:37.900+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:30:58.482647+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:01:37.900+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:30:58.482647+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:37.900+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:01:37.900+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:37.908+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:38.882+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:01:42.372+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:20:00.876832+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:01:51.553+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:30:58.482647+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:01:52.547+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:01:56.002+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:30:58.482647+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:02:05.162+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:02:06.155+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:02:09.710+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:02:13.843+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T16:02:13.843+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:30:58.482647+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:02:13.843+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:02:13.849+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:32:09.754429+00:00, run_end_date=2024-05-18 10:32:12.552002+00:00, run_duration=2.797573, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:31:37.898367+00:00, queued_by_job_id=18, pid=77599[0m
[[34m2024-05-18T16:02:13.849+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:30:58.482647+00:00, map_index=-1, run_start_date=2024-05-18 10:31:56.043261+00:00, run_end_date=2024-05-18 10:32:03.892864+00:00, run_duration=7.849603, state=success, executor_state=success, try_number=1, max_tries=3, job_id=24, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:31:37.898367+00:00, queued_by_job_id=18, pid=77507[0m
[[34m2024-05-18T16:02:13.849+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:20:00.876832+00:00, map_index=-1, run_start_date=2024-05-18 10:31:42.418781+00:00, run_end_date=2024-05-18 10:31:50.248998+00:00, run_duration=7.830217, state=success, executor_state=success, try_number=2, max_tries=4, job_id=23, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:31:37.898367+00:00, queued_by_job_id=18, pid=77368[0m
[[34m2024-05-18T16:02:14.065+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:30:58.482647+00:00 [scheduled]>[0m
[[34m2024-05-18T16:02:14.065+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:02:14.065+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:02:14.065+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:30:58.482647+00:00 [scheduled]>[0m
[[34m2024-05-18T16:02:14.068+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:02:14.068+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:02:14.068+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:30:58.482647+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:02:14.068+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:30:58.482647+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:02:14.077+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:02:15.042+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:02:18.536+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:02:22.558+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:30:58.482647+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:02:23.526+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:02:27.071+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:30:58.482647+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:02:31.520+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T16:02:31.521+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:30:58.482647+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:02:31.527+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:20:00.876832+00:00, map_index=-1, run_start_date=2024-05-18 10:32:18.580277+00:00, run_end_date=2024-05-18 10:32:21.295261+00:00, run_duration=2.714984, state=up_for_retry, executor_state=success, try_number=3, max_tries=5, job_id=26, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:32:14.066489+00:00, queued_by_job_id=18, pid=77637[0m
[[34m2024-05-18T16:02:31.527+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:30:58.482647+00:00, map_index=-1, run_start_date=2024-05-18 10:32:27.113690+00:00, run_end_date=2024-05-18 10:32:30.238301+00:00, run_duration=3.124611, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=27, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:32:14.066489+00:00, queued_by_job_id=18, pid=77694[0m
[[34m2024-05-18T16:03:10.603+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:33:09.970571+00:00 [scheduled]>[0m
[[34m2024-05-18T16:03:10.603+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:03:10.604+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:33:09.970571+00:00 [scheduled]>[0m
[[34m2024-05-18T16:03:10.605+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:33:09.970571+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:03:10.605+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:33:09.970571+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:03:10.614+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:33:09.970571+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:03:11.652+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:03:15.108+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:33:09.970571+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:03:20.881+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:33:09.970571+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:03:20.884+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:33:09.970571+00:00, map_index=-1, run_start_date=2024-05-18 10:33:15.148939+00:00, run_end_date=2024-05-18 10:33:19.671504+00:00, run_duration=4.522565, state=success, executor_state=success, try_number=1, max_tries=3, job_id=28, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:33:10.604668+00:00, queued_by_job_id=18, pid=77903[0m
[[34m2024-05-18T16:03:21.089+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:33:09.970571+00:00 [scheduled]>[0m
[[34m2024-05-18T16:03:21.089+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:03:21.090+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:33:09.970571+00:00 [scheduled]>[0m
[[34m2024-05-18T16:03:21.091+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:33:09.970571+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:03:21.091+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:33:09.970571+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:03:21.100+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:33:09.970571+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:03:22.052+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:03:25.532+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:33:09.970571+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:03:34.577+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:33:09.970571+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:03:34.580+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:33:09.970571+00:00, map_index=-1, run_start_date=2024-05-18 10:33:25.572484+00:00, run_end_date=2024-05-18 10:33:33.344595+00:00, run_duration=7.772111, state=success, executor_state=success, try_number=1, max_tries=3, job_id=29, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:33:21.090538+00:00, queued_by_job_id=18, pid=77989[0m
[[34m2024-05-18T16:03:34.766+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:33:09.970571+00:00 [scheduled]>[0m
[[34m2024-05-18T16:03:34.766+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:03:34.766+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:33:09.970571+00:00 [scheduled]>[0m
[[34m2024-05-18T16:03:34.768+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:33:09.970571+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:03:34.768+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:33:09.970571+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:03:34.776+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:33:09.970571+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:03:35.748+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:03:39.224+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:33:09.970571+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:03:43.335+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:33:09.970571+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:03:43.339+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:33:09.970571+00:00, map_index=-1, run_start_date=2024-05-18 10:33:39.264729+00:00, run_end_date=2024-05-18 10:33:42.073916+00:00, run_duration=2.809187, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=30, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:33:34.767268+00:00, queued_by_job_id=18, pid=78033[0m
[[34m2024-05-18T16:04:18.475+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:04:18.476+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:04:18.476+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:04:18.477+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:04:18.478+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:04:18.486+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:04:19.469+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:04:23.007+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:04:29.035+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T16:04:29.039+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:34:23.048289+00:00, run_end_date=2024-05-18 10:34:27.778755+00:00, run_duration=4.730466, state=success, executor_state=success, try_number=2, max_tries=4, job_id=31, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:34:18.476820+00:00, queued_by_job_id=18, pid=78387[0m
[[34m2024-05-18T16:04:29.254+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:04:29.254+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:04:29.255+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:04:29.256+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:04:29.256+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:04:29.265+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:04:30.217+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:04:33.676+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:04:42.824+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T16:04:42.828+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:34:33.718247+00:00, run_end_date=2024-05-18 10:34:41.585819+00:00, run_duration=7.867572, state=success, executor_state=success, try_number=2, max_tries=4, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:34:29.255667+00:00, queued_by_job_id=18, pid=78527[0m
[[34m2024-05-18T16:04:43.028+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:04:43.028+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:04:43.029+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:04:43.030+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:04:43.030+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:04:43.039+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:04:44.003+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:04:47.506+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:04:51.725+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T16:04:51.729+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:34:47.556452+00:00, run_end_date=2024-05-18 10:34:50.400731+00:00, run_duration=2.844279, state=up_for_retry, executor_state=success, try_number=2, max_tries=4, job_id=33, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:34:43.029625+00:00, queued_by_job_id=18, pid=78672[0m
[[34m2024-05-18T16:04:51.752+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:07:21.834+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T16:07:21.834+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:07:21.834+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:20:00.876832+00:00 [scheduled]>[0m
[[34m2024-05-18T16:07:21.835+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:07:21.835+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:07:21.844+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:07:22.882+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/autoassign/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 421, in task_run
    ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 179, in _get_ti
    dag_run, dr_created = _get_dag_run(
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 128, in _get_dag_run
    raise DagRunNotFound(
airflow.exceptions.DagRunNotFound: DagRun for sentimental_analysis_training_pipeline with run_id or execution_date of 'manual__2024-05-18T10:20:00.876832+00:00' not found
[[34m2024-05-18T16:07:27.701+0530[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:20:00.876832+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py']' returned non-zero exit status 1..[0m
[[34m2024-05-18T16:07:27.702+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:20:00.876832+00:00', try_number=4, map_index=-1)[0m
[2024-05-18 16:07:29 +0530] [76095] [INFO] Handling signal: int
[[34m2024-05-18T16:07:29.704+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:07:29 +0530] [76096] [INFO] Worker exiting (pid: 76096)
[2024-05-18 16:07:29 +0530] [76098] [INFO] Worker exiting (pid: 76098)
[2024-05-18 16:07:29 +0530] [76095] [INFO] Shutting down: Master
[[34m2024-05-18T16:07:30.719+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 76097. PIDs of all processes in the group: [76097][0m
[[34m2024-05-18T16:07:30.720+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 76097[0m
[[34m2024-05-18T16:07:30.892+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=76097, status='terminated', exitcode=0, started='15:59:50') (76097) terminated with exit code 0[0m
[[34m2024-05-18T16:07:30.907+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 76097. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:07:30.908+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 76097[0m
[[34m2024-05-18T16:07:30.908+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 76097 as process group is missing.[0m
[[34m2024-05-18T16:07:30.908+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:07:33.718+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:07:33.718+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:07:33 +0530] [79378] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:07:33 +0530] [79378] [INFO] Listening at: http://[::]:8793 (79378)
[2024-05-18 16:07:33 +0530] [79378] [INFO] Using worker: sync
[[34m2024-05-18T16:07:33.749+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:07:33.750+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:07:33 +0530] [79379] [INFO] Booting worker with pid: 79379
[[34m2024-05-18T16:07:33.755+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 79380[0m
[[34m2024-05-18T16:07:33.757+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:07:33.760+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:07:33.784+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:07:33 +0530] [79382] [INFO] Booting worker with pid: 79382
[[34m2024-05-18T16:07:50.540+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[[34m2024-05-18T16:07:50.617+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:37:49.363233+00:00 [scheduled]>[0m
[[34m2024-05-18T16:07:50.617+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:07:50.618+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:07:50.618+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:37:49.363233+00:00 [scheduled]>[0m
[[34m2024-05-18T16:07:50.620+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:07:50.620+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:07:50.621+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:37:49.363233+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:07:50.621+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:37:49.363233+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:07:50.630+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:07:51.601+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:07:55.066+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:08:00.821+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:37:49.363233+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:01.783+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:08:05.218+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:37:49.363233+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:08:11.022+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:08:11.023+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:37:49.363233+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:08:11.030+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:37:49.363233+00:00, map_index=-1, run_start_date=2024-05-18 10:38:05.258554+00:00, run_end_date=2024-05-18 10:38:09.770509+00:00, run_duration=4.511955, state=success, executor_state=success, try_number=1, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:37:50.618989+00:00, queued_by_job_id=19, pid=79825[0m
[[34m2024-05-18T16:08:11.031+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:37:55.107483+00:00, run_end_date=2024-05-18 10:37:59.616263+00:00, run_duration=4.50878, state=success, executor_state=success, try_number=1, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:37:50.618989+00:00, queued_by_job_id=19, pid=79734[0m
[[34m2024-05-18T16:08:11.297+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:37:49.363233+00:00 [scheduled]>[0m
[[34m2024-05-18T16:08:11.297+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:08:11.297+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:08:11.298+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:37:49.363233+00:00 [scheduled]>[0m
[[34m2024-05-18T16:08:11.299+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:08:11.299+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:11.300+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:37:49.363233+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:08:11.300+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:37:49.363233+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:11.309+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:12.285+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:08:15.765+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:08:24.857+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:37:49.363233+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:25.824+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:08:29.280+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:37:49.363233+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:08:38.459+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:08:38.460+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:37:49.363233+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:08:38.464+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:37:49.363233+00:00, map_index=-1, run_start_date=2024-05-18 10:38:29.320162+00:00, run_end_date=2024-05-18 10:38:37.236311+00:00, run_duration=7.916149, state=success, executor_state=success, try_number=1, max_tries=3, job_id=23, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:38:11.298574+00:00, queued_by_job_id=19, pid=79911[0m
[[34m2024-05-18T16:08:38.464+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:38:15.807817+00:00, run_end_date=2024-05-18 10:38:23.650652+00:00, run_duration=7.842835, state=success, executor_state=success, try_number=1, max_tries=3, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:38:11.298574+00:00, queued_by_job_id=19, pid=79862[0m
[[34m2024-05-18T16:08:38.651+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:37:49.363233+00:00 [scheduled]>[0m
[[34m2024-05-18T16:08:38.652+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:08:38.652+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:08:38.652+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:37:49.363233+00:00 [scheduled]>[0m
[[34m2024-05-18T16:08:38.653+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:08:38.653+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:38.654+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:37:49.363233+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:08:38.654+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:37:49.363233+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:38.663+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:39.632+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:08:43.118+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:08:48.879+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:37:49.363233+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:08:49.855+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:08:53.297+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:37:49.363233+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:08:57.296+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:08:57.296+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:37:49.363233+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:08:57.301+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:37:49.363233+00:00, map_index=-1, run_start_date=2024-05-18 10:38:53.337418+00:00, run_end_date=2024-05-18 10:38:56.081940+00:00, run_duration=2.744522, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:38:38.652976+00:00, queued_by_job_id=19, pid=80042[0m
[[34m2024-05-18T16:08:57.301+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:38:43.160813+00:00, run_end_date=2024-05-18 10:38:47.654289+00:00, run_duration=4.493476, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:38:38.652976+00:00, queued_by_job_id=19, pid=80004[0m
[[34m2024-05-18T16:10:33.788+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:10:33 +0530] [79378] [INFO] Handling signal: int
[2024-05-18 16:10:33 +0530] [79382] [INFO] Worker exiting (pid: 79382)
[2024-05-18 16:10:33 +0530] [79379] [INFO] Worker exiting (pid: 79379)
[2024-05-18 16:10:33 +0530] [79378] [INFO] Shutting down: Master
[[34m2024-05-18T16:10:34.289+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:10:34.317+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:10:34.347+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:10:34.378+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:10:34.410+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:10:34.438+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:10:34.468+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:10:35.482+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 79380. PIDs of all processes in the group: [79380][0m
[[34m2024-05-18T16:10:35.482+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 79380[0m
[[34m2024-05-18T16:10:35.655+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=79380, status='terminated', exitcode=0, started='16:07:33') (79380) terminated with exit code 0[0m
[[34m2024-05-18T16:10:35.655+0530[0m] {[34mbase.py:[0m264} ERROR[0m - Exception closing connection <sqlite3.Connection object at 0x7b7e31938540>[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 262, in _close_connection
    self._dialect.do_close(connection)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 692, in do_close
    dbapi_connection.close()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 258, in _exit_gracefully
    self.processor_agent.end()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/dag_processing/manager.py", line 326, in end
    self._process.join(timeout=1.0)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 258, in _exit_gracefully
    self.processor_agent.end()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/dag_processing/manager.py", line 326, in end
    self._process.join(timeout=1.0)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 258, in _exit_gracefully
    self.processor_agent.end()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/dag_processing/manager.py", line 326, in end
    self._process.join(timeout=1.0)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 258, in _exit_gracefully
    self.processor_agent.end()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/dag_processing/manager.py", line 326, in end
    self._process.join(timeout=1.0)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 258, in _exit_gracefully
    self.processor_agent.end()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/dag_processing/manager.py", line 326, in end
    self._process.join(timeout=1.0)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 258, in _exit_gracefully
    self.processor_agent.end()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/dag_processing/manager.py", line 326, in end
    self._process.join(timeout=1.0)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 258, in _exit_gracefully
    self.processor_agent.end()
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/dag_processing/manager.py", line 326, in end
    self._process.join(timeout=1.0)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/skumar/miniconda3/envs/autoassign/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 259, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0
[[34m2024-05-18T16:10:35.677+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 79380. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:10:35.677+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 79380[0m
[[34m2024-05-18T16:10:35.677+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 79380 as process group is missing.[0m
[[34m2024-05-18T16:10:35.677+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:10:47.904+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:10:47.905+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:10:47 +0530] [80585] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:10:47 +0530] [80585] [INFO] Listening at: http://[::]:8793 (80585)
[2024-05-18 16:10:47 +0530] [80585] [INFO] Using worker: sync
[[34m2024-05-18T16:10:47.937+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:10:47.937+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:10:47 +0530] [80586] [INFO] Booting worker with pid: 80586
[[34m2024-05-18T16:10:47.943+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 80587[0m
[[34m2024-05-18T16:10:47.945+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:10:47.948+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:10:47.971+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:10:48 +0530] [80588] [INFO] Booting worker with pid: 80588
[[34m2024-05-18T16:11:07.888+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:11:07 +0530] [80585] [INFO] Handling signal: int
[2024-05-18 16:11:07 +0530] [80588] [INFO] Worker exiting (pid: 80588)
[2024-05-18 16:11:07 +0530] [80586] [INFO] Worker exiting (pid: 80586)
[2024-05-18 16:11:08 +0530] [80585] [INFO] Shutting down: Master
[[34m2024-05-18T16:11:08.585+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:11:09.104+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:11:10.115+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 80587. PIDs of all processes in the group: [80587][0m
[[34m2024-05-18T16:11:10.116+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 80587[0m
[[34m2024-05-18T16:11:10.288+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=80587, status='terminated', exitcode=0, started='16:10:47') (80587) terminated with exit code 0[0m
[[34m2024-05-18T16:11:10.305+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 80587. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:11:10.305+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 80587[0m
[[34m2024-05-18T16:11:10.305+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 80587 as process group is missing.[0m
[[34m2024-05-18T16:11:10.305+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:11:31.553+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:11:31.553+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:11:31 +0530] [80947] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:11:31 +0530] [80947] [INFO] Listening at: http://[::]:8793 (80947)
[2024-05-18 16:11:31 +0530] [80947] [INFO] Using worker: sync
[[34m2024-05-18T16:11:31.584+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:11:31.585+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:11:31 +0530] [80948] [INFO] Booting worker with pid: 80948
[[34m2024-05-18T16:11:31.591+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 80949[0m
[[34m2024-05-18T16:11:31.592+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:11:31.596+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:11:31.620+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:11:31 +0530] [80950] [INFO] Booting worker with pid: 80950
[2024-05-18 16:11:33 +0530] [80947] [INFO] Handling signal: winch
[2024-05-18 16:12:14 +0530] [80947] [INFO] Handling signal: int
[[34m2024-05-18T16:12:14.551+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:12:14 +0530] [80948] [INFO] Worker exiting (pid: 80948)
[2024-05-18 16:12:14 +0530] [80950] [INFO] Worker exiting (pid: 80950)
[2024-05-18 16:12:14 +0530] [80947] [INFO] Shutting down: Master
[[34m2024-05-18T16:12:14.938+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:12:15.183+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:12:15.367+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:12:16.380+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 80949. PIDs of all processes in the group: [80949][0m
[[34m2024-05-18T16:12:16.381+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 80949[0m
[[34m2024-05-18T16:12:16.553+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=80949, status='terminated', exitcode=0, started='16:11:31') (80949) terminated with exit code 0[0m
[[34m2024-05-18T16:12:16.571+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 80949. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:12:16.571+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 80949[0m
[[34m2024-05-18T16:12:16.571+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 80949 as process group is missing.[0m
[[34m2024-05-18T16:12:16.572+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:14:58.691+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:14:58.691+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:14:58 +0530] [81877] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:14:58 +0530] [81877] [INFO] Listening at: http://[::]:8793 (81877)
[2024-05-18 16:14:58 +0530] [81877] [INFO] Using worker: sync
[[34m2024-05-18T16:14:58.723+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:14:58.723+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:14:58 +0530] [81878] [INFO] Booting worker with pid: 81878
[[34m2024-05-18T16:14:58.729+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 81879[0m
[[34m2024-05-18T16:14:58.730+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:14:58.733+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:14:58.756+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:14:58 +0530] [81880] [INFO] Booting worker with pid: 81880
[2024-05-18 16:15:37 +0530] [81877] [INFO] Handling signal: int
[[34m2024-05-18T16:15:37.734+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:15:37 +0530] [81878] [INFO] Worker exiting (pid: 81878)
[2024-05-18 16:15:37 +0530] [81880] [INFO] Worker exiting (pid: 81880)
[2024-05-18 16:15:37 +0530] [81877] [INFO] Shutting down: Master
[[34m2024-05-18T16:15:38.749+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 81879. PIDs of all processes in the group: [81879][0m
[[34m2024-05-18T16:15:38.749+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 81879[0m
[[34m2024-05-18T16:15:38.921+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=81879, status='terminated', exitcode=0, started='16:14:58') (81879) terminated with exit code 0[0m
[[34m2024-05-18T16:15:38.941+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 81879. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:15:38.941+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 81879[0m
[[34m2024-05-18T16:15:38.941+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 81879 as process group is missing.[0m
[[34m2024-05-18T16:15:38.941+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:16:28.751+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:16:28.751+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:16:28 +0530] [82387] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:16:28 +0530] [82387] [INFO] Listening at: http://[::]:8793 (82387)
[2024-05-18 16:16:28 +0530] [82387] [INFO] Using worker: sync
[[34m2024-05-18T16:16:28.782+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:16:28.783+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:16:28 +0530] [82388] [INFO] Booting worker with pid: 82388
[[34m2024-05-18T16:16:28.789+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 82389[0m
[[34m2024-05-18T16:16:28.790+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:16:28.793+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:16:28.817+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:16:28 +0530] [82390] [INFO] Booting worker with pid: 82390
[[34m2024-05-18T16:17:32.738+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:17:32 +0530] [82387] [INFO] Handling signal: int
[2024-05-18 16:17:32 +0530] [82388] [INFO] Worker exiting (pid: 82388)
[2024-05-18 16:17:32 +0530] [82390] [INFO] Worker exiting (pid: 82390)
[2024-05-18 16:17:32 +0530] [82387] [INFO] Shutting down: Master
[[34m2024-05-18T16:17:33.748+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 82389. PIDs of all processes in the group: [82389][0m
[[34m2024-05-18T16:17:33.749+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 82389[0m
[[34m2024-05-18T16:17:33.921+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=82389, status='terminated', exitcode=0, started='16:16:28') (82389) terminated with exit code 0[0m
[[34m2024-05-18T16:17:33.948+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 82389. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:17:33.948+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 82389[0m
[[34m2024-05-18T16:17:33.949+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 82389 as process group is missing.[0m
[[34m2024-05-18T16:17:33.949+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:17:36.875+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:17:36.875+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:17:36 +0530] [82744] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:17:36 +0530] [82744] [INFO] Listening at: http://[::]:8793 (82744)
[2024-05-18 16:17:36 +0530] [82744] [INFO] Using worker: sync
[[34m2024-05-18T16:17:36.907+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:17:36.908+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:17:36 +0530] [82745] [INFO] Booting worker with pid: 82745
[[34m2024-05-18T16:17:36.914+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 82746[0m
[[34m2024-05-18T16:17:36.915+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:17:36.917+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:17:36.941+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:17:36 +0530] [82747] [INFO] Booting worker with pid: 82747
[2024-05-18 16:17:56 +0530] [82744] [INFO] Handling signal: int
[[34m2024-05-18T16:17:56.134+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:17:56 +0530] [82747] [INFO] Worker exiting (pid: 82747)
[2024-05-18 16:17:56 +0530] [82745] [INFO] Worker exiting (pid: 82745)
[2024-05-18 16:17:56 +0530] [82744] [INFO] Shutting down: Master
[[34m2024-05-18T16:17:56.634+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:17:56.662+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:17:56.693+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:17:56.723+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:17:56.754+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:17:56.784+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:17:56.814+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:17:57.827+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 82746. PIDs of all processes in the group: [82746][0m
[[34m2024-05-18T16:17:57.827+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 82746[0m
[[34m2024-05-18T16:17:58.000+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=82746, status='terminated', exitcode=0, started='16:17:36') (82746) terminated with exit code 0[0m
[[34m2024-05-18T16:17:58.018+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 82746. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:17:58.018+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 82746[0m
[[34m2024-05-18T16:17:58.018+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 82746 as process group is missing.[0m
[[34m2024-05-18T16:17:58.019+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:18:11.328+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:18:11.328+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:18:11 +0530] [83049] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:18:11 +0530] [83049] [INFO] Listening at: http://[::]:8793 (83049)
[2024-05-18 16:18:11 +0530] [83049] [INFO] Using worker: sync
[[34m2024-05-18T16:18:11.360+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:18:11.360+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:18:11 +0530] [83050] [INFO] Booting worker with pid: 83050
[[34m2024-05-18T16:18:11.366+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 83051[0m
[[34m2024-05-18T16:18:11.368+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:18:11.371+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18 16:18:11 +0530] [83052] [INFO] Booting worker with pid: 83052
[2024-05-18T16:18:11.395+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:18:46 +0530] [83049] [INFO] Handling signal: int
[[34m2024-05-18T16:18:46.546+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:18:46 +0530] [83052] [INFO] Worker exiting (pid: 83052)
[2024-05-18 16:18:46 +0530] [83050] [INFO] Worker exiting (pid: 83050)
[2024-05-18 16:18:46 +0530] [83049] [INFO] Shutting down: Master
[[34m2024-05-18T16:18:47.555+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 83051. PIDs of all processes in the group: [83051][0m
[[34m2024-05-18T16:18:47.556+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 83051[0m
[[34m2024-05-18T16:18:47.728+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=83051, status='terminated', exitcode=0, started='16:18:11') (83051) terminated with exit code 0[0m
[[34m2024-05-18T16:18:47.746+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 83051. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:18:47.747+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 83051[0m
[[34m2024-05-18T16:18:47.747+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 83051 as process group is missing.[0m
[[34m2024-05-18T16:18:47.747+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:18:59.132+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:18:59.133+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:18:59 +0530] [83506] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:18:59 +0530] [83506] [INFO] Listening at: http://[::]:8793 (83506)
[2024-05-18 16:18:59 +0530] [83506] [INFO] Using worker: sync
[2024-05-18 16:18:59 +0530] [83507] [INFO] Booting worker with pid: 83507
[2024-05-18 16:18:59 +0530] [83508] [INFO] Booting worker with pid: 83508
[[34m2024-05-18T16:18:59.182+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:18:59.183+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-05-18T16:18:59.189+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 83509[0m
[[34m2024-05-18T16:18:59.190+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:18:59.193+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:18:59.216+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-18T16:19:19.037+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 16:19:19 +0530] [83506] [INFO] Handling signal: int
[2024-05-18 16:19:19 +0530] [83508] [INFO] Worker exiting (pid: 83508)
[2024-05-18 16:19:19 +0530] [83507] [INFO] Worker exiting (pid: 83507)
[2024-05-18 16:19:19 +0530] [83506] [INFO] Shutting down: Master
[[34m2024-05-18T16:19:19.539+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:19:19.567+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:19:19.597+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:19:19.628+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:19:19.659+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:19:19.689+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:19:19.720+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T16:19:20.733+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 83509. PIDs of all processes in the group: [83509][0m
[[34m2024-05-18T16:19:20.734+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 83509[0m
[[34m2024-05-18T16:19:20.906+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=83509, status='terminated', exitcode=0, started='16:18:59') (83509) terminated with exit code 0[0m
[[34m2024-05-18T16:19:20.924+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 83509. PIDs of all processes in the group: [][0m
[[34m2024-05-18T16:19:20.924+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 83509[0m
[[34m2024-05-18T16:19:20.924+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 83509 as process group is missing.[0m
[[34m2024-05-18T16:19:20.924+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T16:19:34.008+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T16:19:34.009+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 16:19:34 +0530] [83911] [INFO] Starting gunicorn 22.0.0
[2024-05-18 16:19:34 +0530] [83911] [INFO] Listening at: http://[::]:8793 (83911)
[2024-05-18 16:19:34 +0530] [83911] [INFO] Using worker: sync
[[34m2024-05-18T16:19:34.041+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T16:19:34.041+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 16:19:34 +0530] [83912] [INFO] Booting worker with pid: 83912
[[34m2024-05-18T16:19:34.047+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 83913[0m
[[34m2024-05-18T16:19:34.048+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:19:34.051+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T16:19:34.074+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 16:19:34 +0530] [83915] [INFO] Booting worker with pid: 83915
[[34m2024-05-18T16:19:48.026+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[[34m2024-05-18T16:19:48.087+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:19:48.087+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:19:48.087+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:19:48.087+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:19:48.089+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:19:48.089+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:19:48.089+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T16:19:48.089+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:19:48.097+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:19:49.133+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:19:52.700+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:19:58.534+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:19:59.501+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:20:02.977+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:20:08.768+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:20:08.769+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:20:08.775+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 10:50:03.017871+00:00, run_end_date=2024-05-18 10:50:07.541444+00:00, run_duration=4.523573, state=success, executor_state=success, try_number=1, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:49:48.088019+00:00, queued_by_job_id=6, pid=84382[0m
[[34m2024-05-18T16:20:08.775+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:49:52.742150+00:00, run_end_date=2024-05-18 10:49:57.303070+00:00, run_duration=4.56092, state=success, executor_state=success, try_number=1, max_tries=3, job_id=7, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 10:49:48.088019+00:00, queued_by_job_id=6, pid=84243[0m
[[34m2024-05-18T16:20:08.979+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:20:08.979+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:20:08.979+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:20:08.980+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:20:08.981+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:20:08.981+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:08.981+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T16:20:08.981+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:08.998+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:09.992+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:20:13.478+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:20:22.604+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:23.574+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:20:27.077+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T16:20:36.169+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:20:36.170+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:20:36.172+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 10:50:27.127557+00:00, run_end_date=2024-05-18 10:50:34.901680+00:00, run_duration=7.774123, state=success, executor_state=success, try_number=1, max_tries=3, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:50:08.980280+00:00, queued_by_job_id=6, pid=84692[0m
[[34m2024-05-18T16:20:36.172+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:50:13.520916+00:00, run_end_date=2024-05-18 10:50:21.297507+00:00, run_duration=7.776591, state=success, executor_state=success, try_number=1, max_tries=3, job_id=9, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 10:50:08.980280+00:00, queued_by_job_id=6, pid=84526[0m
[[34m2024-05-18T16:20:36.366+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:20:36.366+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:20:36.366+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T16:20:36.366+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:20:36.367+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:20:36.368+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:36.368+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:20:36.368+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:36.376+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:37.366+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:20:40.858+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:20:45.376+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:20:46.381+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:20:49.882+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:20:53.902+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:20:53.902+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T16:20:53.905+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 10:50:49.925000+00:00, run_end_date=2024-05-18 10:50:52.647017+00:00, run_duration=2.722017, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:50:36.367225+00:00, queued_by_job_id=6, pid=84995[0m
[[34m2024-05-18T16:20:53.905+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:50:40.902643+00:00, run_end_date=2024-05-18 10:50:44.129225+00:00, run_duration=3.226582, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:50:36.367225+00:00, queued_by_job_id=6, pid=84862[0m
[[34m2024-05-18T16:24:34.278+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:25:44.657+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:25:44.658+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:25:44.658+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:25:44.660+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:25:44.660+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:25:44.669+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:25:45.650+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:25:49.143+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[2024-05-18 16:25:50 +0530] [83911] [INFO] Handling signal: winch
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:25:54.519+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T16:25:54.527+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 10:55:49.193047+00:00, run_end_date=2024-05-18 10:55:53.277902+00:00, run_duration=4.084855, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:55:44.658778+00:00, queued_by_job_id=6, pid=88476[0m
[[34m2024-05-18T16:25:54.809+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:25:54.809+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:25:54.809+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:25:54.810+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:25:54.810+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:25:54.818+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:25:55.792+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:25:59.241+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:26:03.590+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T16:26:03.594+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 10:55:59.281125+00:00, run_end_date=2024-05-18 10:56:02.374120+00:00, run_duration=3.092995, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 10:55:54.809767+00:00, queued_by_job_id=6, pid=88631[0m
[[34m2024-05-18T16:29:34.487+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:30:53.510+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:30:53.510+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:30:53.511+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:30:53.512+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:30:53.512+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:30:53.520+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:30:54.474+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:30:57.888+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:31:03.217+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T16:31:03.220+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 11:00:57.930474+00:00, run_end_date=2024-05-18 11:01:02.004862+00:00, run_duration=4.074388, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 11:00:53.511709+00:00, queued_by_job_id=6, pid=92097[0m
[[34m2024-05-18T16:31:03.425+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:31:03.425+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:31:03.425+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:31:03.427+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:31:03.427+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:31:03.436+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:31:04.397+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:31:07.788+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:31:13.479+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T16:31:13.483+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 11:01:07.827814+00:00, run_end_date=2024-05-18 11:01:12.232833+00:00, run_duration=4.405019, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 11:01:03.426496+00:00, queued_by_job_id=6, pid=92236[0m
[[34m2024-05-18T16:34:34.584+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:36:02.145+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:36:02.145+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:36:02.145+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T16:36:02.147+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:36:02.147+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:36:02.156+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:36:03.113+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:36:06.555+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:36:10.623+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-05-18T16:36:10.627+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 11:06:06.596437+00:00, run_end_date=2024-05-18 11:06:09.387662+00:00, run_duration=2.791225, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 11:06:02.146385+00:00, queued_by_job_id=6, pid=95666[0m
[[34m2024-05-18T16:36:10.782+0530[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun sentimental_analysis_training_pipeline @ 2024-05-05 00:00:00+00:00: scheduled__2024-05-05T00:00:00+00:00, state:running, queued_at: 2024-05-18 10:49:48.017719+00:00. externally triggered: False> failed[0m
[[34m2024-05-18T16:36:10.782+0530[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=sentimental_analysis_training_pipeline, execution_date=2024-05-05 00:00:00+00:00, run_id=scheduled__2024-05-05T00:00:00+00:00, run_start_date=2024-05-18 10:49:48.041928+00:00, run_end_date=2024-05-18 11:06:10.782827+00:00, run_duration=982.740899, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-12 00:00:00+00:00, dag_hash=dfae8fa89ec57ec058f50008e44c7e8e[0m
[[34m2024-05-18T16:36:10.788+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[[34m2024-05-18T16:36:13.147+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:36:13.147+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T16:36:13.148+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T16:36:13.149+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T16:36:13.149+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:36:13.158+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T16:36:14.096+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T16:36:17.500+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T16:36:21.913+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=4, map_index=-1)[0m
[[34m2024-05-18T16:36:21.918+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 11:06:17.540960+00:00, run_end_date=2024-05-18 11:06:20.690361+00:00, run_duration=3.149401, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 11:06:13.148635+00:00, queued_by_job_id=6, pid=95829[0m
[[34m2024-05-18T16:36:25.231+0530[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun sentimental_analysis_training_pipeline @ 2024-05-18 10:49:47.244537+00:00: manual__2024-05-18T10:49:47.244537+00:00, state:running, queued_at: 2024-05-18 10:49:47.268732+00:00. externally triggered: True> failed[0m
[[34m2024-05-18T16:36:25.232+0530[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=sentimental_analysis_training_pipeline, execution_date=2024-05-18 10:49:47.244537+00:00, run_id=manual__2024-05-18T10:49:47.244537+00:00, run_start_date=2024-05-18 10:49:48.042257+00:00, run_end_date=2024-05-18 11:06:25.231948+00:00, run_duration=997.189691, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-12 00:00:00+00:00, dag_hash=dfae8fa89ec57ec058f50008e44c7e8e[0m
[[34m2024-05-18T16:39:35.610+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:44:35.753+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:49:35.831+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:54:37.403+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T16:59:39.273+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:04:39.517+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:09:39.661+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:14:39.804+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:19:42.600+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:24:42.744+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:29:42.888+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:34:43.030+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:39:43.054+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:44:43.198+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:49:43.363+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:54:43.506+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T17:59:43.650+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:04:43.793+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:09:43.936+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:14:44.180+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:19:44.296+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:24:44.440+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 18:27:19 +0530] [83911] [INFO] Handling signal: winch
[[34m2024-05-18T18:29:44.585+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 18:30:03 +0530] [83911] [INFO] Handling signal: int
[[34m2024-05-18T18:30:03.000+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 18:30:03 +0530] [83912] [INFO] Worker exiting (pid: 83912)
[2024-05-18 18:30:03 +0530] [83915] [INFO] Worker exiting (pid: 83915)
[2024-05-18 18:30:03 +0530] [83911] [INFO] Shutting down: Master
[[34m2024-05-18T18:30:04.016+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 83913. PIDs of all processes in the group: [83913][0m
[[34m2024-05-18T18:30:04.016+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 83913[0m
[[34m2024-05-18T18:30:04.148+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=83913, status='terminated', exitcode=0, started='16:19:33') (83913) terminated with exit code 0[0m
[[34m2024-05-18T18:30:04.167+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 83913. PIDs of all processes in the group: [][0m
[[34m2024-05-18T18:30:04.168+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 83913[0m
[[34m2024-05-18T18:30:04.168+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 83913 as process group is missing.[0m
[[34m2024-05-18T18:30:04.168+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T18:30:07.402+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T18:30:07.403+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 18:30:07 +0530] [109669] [INFO] Starting gunicorn 22.0.0
[2024-05-18 18:30:07 +0530] [109669] [INFO] Listening at: http://[::]:8793 (109669)
[2024-05-18 18:30:07 +0530] [109669] [INFO] Using worker: sync
[[34m2024-05-18T18:30:07.434+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T18:30:07.434+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 18:30:07 +0530] [109670] [INFO] Booting worker with pid: 109670
[[34m2024-05-18T18:30:07.440+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 109671[0m
[[34m2024-05-18T18:30:07.442+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:30:07.445+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T18:30:07.470+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 18:30:07 +0530] [109672] [INFO] Booting worker with pid: 109672
[[34m2024-05-18T18:30:26.669+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:30:26.669+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:30:26.669+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:30:26.672+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T18:30:26.672+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:30:26.681+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:30:27.671+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:30:31.296+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T13:00:25.269114+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:30:37.180+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:30:37.188+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T13:00:25.269114+00:00, map_index=-1, run_start_date=2024-05-18 13:00:31.338179+00:00, run_end_date=2024-05-18 13:00:35.940419+00:00, run_duration=4.60224, state=success, executor_state=success, try_number=1, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 13:00:26.670499+00:00, queued_by_job_id=19, pid=110057[0m
[[34m2024-05-18T18:30:37.377+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:30:37.377+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:30:37.378+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:30:37.379+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T18:30:37.380+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:30:37.389+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:30:38.403+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:30:41.864+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T13:00:25.269114+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:30:50.657+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:30:50.661+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T13:00:25.269114+00:00, map_index=-1, run_start_date=2024-05-18 13:00:41.902892+00:00, run_end_date=2024-05-18 13:00:49.426844+00:00, run_duration=7.523952, state=success, executor_state=success, try_number=1, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 13:00:37.378760+00:00, queued_by_job_id=19, pid=110197[0m
[[34m2024-05-18T18:30:50.870+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:30:50.870+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:30:50.870+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:30:50.872+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:30:50.872+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:30:50.881+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:30:51.876+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:30:55.427+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:30:59.256+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:30:59.260+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:00:25.269114+00:00, map_index=-1, run_start_date=2024-05-18 13:00:55.468636+00:00, run_end_date=2024-05-18 13:00:58.009410+00:00, run_duration=2.540774, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:00:50.871416+00:00, queued_by_job_id=19, pid=110400[0m
[[34m2024-05-18T18:33:11.989+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:33:11.990+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:33:11.990+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:33:11.991+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T18:33:11.992+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:12.003+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:13.045+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:33:16.798+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:33:23.105+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T18:33:23.109+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:03:16.847347+00:00, run_end_date=2024-05-18 13:03:21.772522+00:00, run_duration=4.925175, state=success, executor_state=success, try_number=2, max_tries=4, job_id=23, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 13:03:11.990942+00:00, queued_by_job_id=19, pid=112221[0m
[[34m2024-05-18T18:33:23.315+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:33:23.315+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:33:23.316+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T18:33:23.316+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:33:23.318+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T18:33:23.319+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:23.319+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T18:33:23.319+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:23.327+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:24.328+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:33:27.904+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:33:34.123+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:35.146+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:33:38.755+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:33:47.895+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T18:33:47.896+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T18:33:47.904+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:03:38.798957+00:00, run_end_date=2024-05-18 13:03:46.575123+00:00, run_duration=7.776166, state=success, executor_state=success, try_number=2, max_tries=4, job_id=25, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 13:03:23.317028+00:00, queued_by_job_id=19, pid=112662[0m
[[34m2024-05-18T18:33:47.905+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 13:03:27.946103+00:00, run_end_date=2024-05-18 13:03:32.793811+00:00, run_duration=4.847708, state=success, executor_state=success, try_number=2, max_tries=4, job_id=24, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 13:03:23.317028+00:00, queued_by_job_id=19, pid=112457[0m
[[34m2024-05-18T18:33:48.108+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:33:48.109+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:33:48.109+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T18:33:48.109+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:33:48.111+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T18:33:48.111+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:48.112+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=5, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:33:48.112+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:48.121+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:33:49.121+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:33:52.683+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:34:01.692+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:34:02.663+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:34:06.266+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:34:11.378+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T18:34:11.378+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=5, map_index=-1)[0m
[[34m2024-05-18T18:34:11.381+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:04:06.316253+00:00, run_end_date=2024-05-18 13:04:10.133237+00:00, run_duration=3.816984, state=up_for_retry, executor_state=success, try_number=5, max_tries=7, job_id=27, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:03:48.110502+00:00, queued_by_job_id=19, pid=113114[0m
[[34m2024-05-18T18:34:11.381+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 13:03:52.726881+00:00, run_end_date=2024-05-18 13:04:00.414698+00:00, run_duration=7.687817, state=success, executor_state=success, try_number=2, max_tries=4, job_id=26, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 13:03:48.110502+00:00, queued_by_job_id=19, pid=112927[0m
[[34m2024-05-18T18:34:11.692+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T18:34:11.692+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:34:11.692+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T18:34:11.694+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=5, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:34:11.694+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:34:11.703+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:34:12.670+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:34:16.176+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:34:20.821+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=5, map_index=-1)[0m
[[34m2024-05-18T18:34:20.825+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 13:04:16.220508+00:00, run_end_date=2024-05-18 13:04:19.581025+00:00, run_duration=3.360517, state=up_for_retry, executor_state=success, try_number=5, max_tries=7, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:04:11.693374+00:00, queued_by_job_id=19, pid=113208[0m
[[34m2024-05-18T18:35:07.580+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:35:59.020+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:35:59.020+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:35:59.020+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:35:59.022+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:35:59.022+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:35:59.031+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:35:59.988+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:36:03.361+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:36:08.227+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T18:36:08.230+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:00:25.269114+00:00, map_index=-1, run_start_date=2024-05-18 13:06:03.400931+00:00, run_end_date=2024-05-18 13:06:07.028796+00:00, run_duration=3.627865, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=29, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:05:59.021424+00:00, queued_by_job_id=19, pid=113485[0m
[[34m2024-05-18T18:39:11.111+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:39:11.111+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:39:11.111+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:39:11.112+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=6, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:39:11.112+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:39:11.121+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:39:12.064+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:39:15.439+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:39:19.170+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=6, map_index=-1)[0m
[[34m2024-05-18T18:39:19.173+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:09:15.481101+00:00, run_end_date=2024-05-18 13:09:17.979798+00:00, run_duration=2.498697, state=up_for_retry, executor_state=success, try_number=6, max_tries=7, job_id=30, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:09:11.112250+00:00, queued_by_job_id=19, pid=114034[0m
[[34m2024-05-18T18:39:20.547+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T18:39:20.548+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:39:20.548+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T18:39:20.549+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=6, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:39:20.549+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:39:20.558+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:39:21.516+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:39:24.891+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:39:29.082+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=6, map_index=-1)[0m
[[34m2024-05-18T18:39:29.086+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 13:09:24.931657+00:00, run_end_date=2024-05-18 13:09:27.854143+00:00, run_duration=2.922486, state=up_for_retry, executor_state=success, try_number=6, max_tries=7, job_id=31, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:09:20.548837+00:00, queued_by_job_id=19, pid=114073[0m
[[34m2024-05-18T18:40:07.747+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:41:07.116+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:41:07.116+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:41:07.117+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:41:07.118+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:41:07.118+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:41:07.126+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:41:08.096+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:41:11.487+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:41:15.217+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T18:41:15.221+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:00:25.269114+00:00, map_index=-1, run_start_date=2024-05-18 13:11:11.528038+00:00, run_end_date=2024-05-18 13:11:13.976790+00:00, run_duration=2.448752, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=32, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:11:07.117711+00:00, queued_by_job_id=19, pid=114283[0m
[2024-05-18 18:42:20 +0530] [109669] [INFO] Handling signal: winch
[2024-05-18 18:42:20 +0530] [109669] [INFO] Handling signal: winch
[[34m2024-05-18T18:44:18.646+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:44:18.646+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:44:18.646+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:44:18.648+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=7, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:44:18.648+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:44:18.657+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:44:19.616+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:44:22.989+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:44:26.738+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=7, map_index=-1)[0m
[[34m2024-05-18T18:44:26.742+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:14:23.032038+00:00, run_end_date=2024-05-18 13:14:25.540813+00:00, run_duration=2.508775, state=up_for_retry, executor_state=success, try_number=7, max_tries=7, job_id=33, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:14:18.647302+00:00, queued_by_job_id=19, pid=114861[0m
[[34m2024-05-18T18:44:28.107+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T18:44:28.107+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:44:28.108+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [scheduled]>[0m
[[34m2024-05-18T18:44:28.109+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=7, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:44:28.109+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:44:28.118+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T10:49:47.244537+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:44:29.052+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:44:32.437+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T10:49:47.244537+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:44:36.996+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T10:49:47.244537+00:00', try_number=7, map_index=-1)[0m
[[34m2024-05-18T18:44:36.999+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T10:49:47.244537+00:00, map_index=-1, run_start_date=2024-05-18 13:14:32.477588+00:00, run_end_date=2024-05-18 13:14:35.763135+00:00, run_duration=3.285547, state=up_for_retry, executor_state=success, try_number=7, max_tries=7, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:14:28.108560+00:00, queued_by_job_id=19, pid=114900[0m
[[34m2024-05-18T18:45:07.875+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 18:46:10 +0530] [109669] [INFO] Handling signal: int
[[34m2024-05-18T18:46:10.660+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 18:46:10 +0530] [109670] [INFO] Worker exiting (pid: 109670)
[2024-05-18 18:46:10 +0530] [109672] [INFO] Worker exiting (pid: 109672)
[2024-05-18 18:46:10 +0530] [109669] [INFO] Shutting down: Master
[[34m2024-05-18T18:46:11.161+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:11.190+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:11.221+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:11.250+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:11.281+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:11.310+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:11.341+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:12.354+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 109671. PIDs of all processes in the group: [109671][0m
[[34m2024-05-18T18:46:12.354+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 109671[0m
[[34m2024-05-18T18:46:12.527+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=109671, status='terminated', exitcode=0, started='18:30:07') (109671) terminated with exit code 0[0m
[[34m2024-05-18T18:46:12.545+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 109671. PIDs of all processes in the group: [][0m
[[34m2024-05-18T18:46:12.545+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 109671[0m
[[34m2024-05-18T18:46:12.546+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 109671 as process group is missing.[0m
[[34m2024-05-18T18:46:12.546+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T18:46:15.915+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T18:46:15.915+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 18:46:15 +0530] [115129] [INFO] Starting gunicorn 22.0.0
[2024-05-18 18:46:15 +0530] [115129] [INFO] Listening at: http://[::]:8793 (115129)
[2024-05-18 18:46:15 +0530] [115129] [INFO] Using worker: sync
[2024-05-18 18:46:15 +0530] [115130] [INFO] Booting worker with pid: 115130
[[34m2024-05-18T18:46:15.960+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T18:46:15.960+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-05-18T18:46:15.966+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 115131[0m
[[34m2024-05-18T18:46:15.967+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:46:15.970+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T18:46:15.995+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 18:46:16 +0530] [115133] [INFO] Booting worker with pid: 115133
[[34m2024-05-18T18:46:19.278+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:46:19.279+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:46:19.279+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [scheduled]>[0m
[[34m2024-05-18T18:46:19.280+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:46:19.280+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:46:19.289+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:00:25.269114+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:46:20.247+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:46:23.898+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:00:25.269114+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:46:27.771+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:00:25.269114+00:00', try_number=4, map_index=-1)[0m
[[34m2024-05-18T18:46:27.776+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:00:25.269114+00:00, map_index=-1, run_start_date=2024-05-18 13:16:23.939947+00:00, run_end_date=2024-05-18 13:16:26.519087+00:00, run_duration=2.57914, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=36, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:16:19.279610+00:00, queued_by_job_id=35, pid=115333[0m
[[34m2024-05-18T18:46:27.937+0530[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun sentimental_analysis_training_pipeline @ 2024-05-18 13:00:25.269114+00:00: manual__2024-05-18T13:00:25.269114+00:00, state:running, queued_at: 2024-05-18 13:00:25.283725+00:00. externally triggered: True> failed[0m
[[34m2024-05-18T18:46:27.937+0530[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=sentimental_analysis_training_pipeline, execution_date=2024-05-18 13:00:25.269114+00:00, run_id=manual__2024-05-18T13:00:25.269114+00:00, run_start_date=2024-05-18 13:00:26.608353+00:00, run_end_date=2024-05-18 13:16:27.937750+00:00, run_duration=961.329397, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-12 00:00:00+00:00, dag_hash=dfae8fa89ec57ec058f50008e44c7e8e[0m
[2024-05-18 18:46:41 +0530] [115129] [INFO] Handling signal: int
[[34m2024-05-18T18:46:41.392+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 18:46:41 +0530] [115133] [INFO] Worker exiting (pid: 115133)
[2024-05-18 18:46:41 +0530] [115130] [INFO] Worker exiting (pid: 115130)
[2024-05-18 18:46:41 +0530] [115129] [INFO] Shutting down: Master
[[34m2024-05-18T18:46:41.892+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:41.921+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:41.951+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:41.981+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:42.011+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:42.041+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-05-18T18:46:43.055+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 115131. PIDs of all processes in the group: [115131][0m
[[34m2024-05-18T18:46:43.056+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 115131[0m
[[34m2024-05-18T18:46:43.188+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=115131, status='terminated', exitcode=0, started='18:46:15') (115131) terminated with exit code 0[0m
[[34m2024-05-18T18:46:43.198+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 115131. PIDs of all processes in the group: [][0m
[[34m2024-05-18T18:46:43.199+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 115131[0m
[[34m2024-05-18T18:46:43.199+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 115131 as process group is missing.[0m
[[34m2024-05-18T18:46:43.199+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T18:46:44.614+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T18:46:44.615+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 18:46:44 +0530] [115552] [INFO] Starting gunicorn 22.0.0
[2024-05-18 18:46:44 +0530] [115552] [INFO] Listening at: http://[::]:8793 (115552)
[2024-05-18 18:46:44 +0530] [115552] [INFO] Using worker: sync
[[34m2024-05-18T18:46:44.645+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T18:46:44.646+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 18:46:44 +0530] [115553] [INFO] Booting worker with pid: 115553
[[34m2024-05-18T18:46:44.651+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 115554[0m
[[34m2024-05-18T18:46:44.653+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:46:44.657+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T18:46:44.680+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 18:46:44 +0530] [115556] [INFO] Booting worker with pid: 115556
[[34m2024-05-18T18:46:58.862+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[[34m2024-05-18T18:46:58.923+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:46:58.924+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:46:58.924+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:46:58.926+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T18:46:58.926+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:46:58.940+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:00.012+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:47:03.491+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:47:09.252+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:47:09.259+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:17:03.535597+00:00, run_end_date=2024-05-18 13:17:08.007311+00:00, run_duration=4.471714, state=success, executor_state=success, try_number=1, max_tries=3, job_id=37, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 13:16:58.924901+00:00, queued_by_job_id=36, pid=115879[0m
[[34m2024-05-18T18:47:09.455+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:47:09.455+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:47:09.455+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T18:47:09.455+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:47:09.458+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-05-18T18:47:09.458+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:09.458+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T18:47:09.459+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:09.467+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_ingestion', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:10.425+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:47:13.884+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_ingestion manual__2024-05-18T13:16:58.217619+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:47:19.737+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:20.710+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:47:24.089+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:47:33.001+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_ingestion', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:47:33.001+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:47:33.006+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:17:24.130833+00:00, run_end_date=2024-05-18 13:17:31.739172+00:00, run_duration=7.608339, state=success, executor_state=success, try_number=1, max_tries=3, job_id=39, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 13:17:09.456663+00:00, queued_by_job_id=36, pid=116158[0m
[[34m2024-05-18T18:47:33.006+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_ingestion, run_id=manual__2024-05-18T13:16:58.217619+00:00, map_index=-1, run_start_date=2024-05-18 13:17:13.925289+00:00, run_end_date=2024-05-18 13:17:18.507637+00:00, run_duration=4.582348, state=success, executor_state=success, try_number=1, max_tries=3, job_id=38, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-05-18 13:17:09.456663+00:00, queued_by_job_id=36, pid=116019[0m
[[34m2024-05-18T18:47:33.204+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:47:33.204+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:47:33.204+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-05-18T18:47:33.205+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:47:33.206+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-18T18:47:33.207+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:33.207+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:47:33.207+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:33.216+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_preprocessing', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:34.172+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:47:37.567+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_preprocessing manual__2024-05-18T13:16:58.217619+00:00 [queued]> on host skumar-pc[0m
[[34m2024-05-18T18:47:46.587+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:47.529+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:47:50.930+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:47:55.474+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_preprocessing', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:47:55.475+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:47:55.478+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:17:50.974368+00:00, run_end_date=2024-05-18 13:17:54.090134+00:00, run_duration=3.115766, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=41, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:17:33.205722+00:00, queued_by_job_id=36, pid=116301[0m
[[34m2024-05-18T18:47:55.478+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_preprocessing, run_id=manual__2024-05-18T13:16:58.217619+00:00, map_index=-1, run_start_date=2024-05-18 13:17:37.607482+00:00, run_end_date=2024-05-18 13:17:45.355485+00:00, run_duration=7.748003, state=success, executor_state=success, try_number=1, max_tries=3, job_id=40, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-18 13:17:33.205722+00:00, queued_by_job_id=36, pid=116230[0m
[[34m2024-05-18T18:47:55.664+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T18:47:55.664+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:47:55.665+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T18:47:55.665+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:47:55.666+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:55.673+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:47:56.867+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:48:00.580+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:48:04.777+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-18T18:48:04.780+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:16:58.217619+00:00, map_index=-1, run_start_date=2024-05-18 13:18:00.626995+00:00, run_end_date=2024-05-18 13:18:03.444399+00:00, run_duration=2.817404, state=up_for_retry, executor_state=success, try_number=1, max_tries=3, job_id=42, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:17:55.665366+00:00, queued_by_job_id=36, pid=116377[0m
[[34m2024-05-18T18:51:44.871+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:52:54.219+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:52:54.219+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:52:54.220+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:52:54.221+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:52:54.221+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:52:54.230+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:52:55.353+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:52:58.996+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:53:03.015+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T18:53:03.020+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:22:59.041576+00:00, run_end_date=2024-05-18 13:23:01.667453+00:00, run_duration=2.625877, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=43, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:22:54.220555+00:00, queued_by_job_id=36, pid=117158[0m
[[34m2024-05-18T18:53:04.864+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T18:53:04.864+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:53:04.865+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T18:53:04.865+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:53:04.865+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:53:04.874+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:53:05.898+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:53:09.535+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:53:14.676+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-18T18:53:14.680+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:16:58.217619+00:00, map_index=-1, run_start_date=2024-05-18 13:23:09.578226+00:00, run_end_date=2024-05-18 13:23:13.299974+00:00, run_duration=3.721748, state=up_for_retry, executor_state=success, try_number=2, max_tries=3, job_id=44, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:23:04.865286+00:00, queued_by_job_id=36, pid=117197[0m
[[34m2024-05-18T18:56:45.045+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T18:58:02.189+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:58:02.189+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:58:02.189+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T18:58:02.191+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:58:02.191+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:58:02.200+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:58:03.212+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:58:06.831+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:58:11.686+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T18:58:11.690+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:28:06.875427+00:00, run_end_date=2024-05-18 13:28:10.338780+00:00, run_duration=3.463353, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=45, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:28:02.190269+00:00, queued_by_job_id=36, pid=117775[0m
[[34m2024-05-18T18:58:14.311+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T18:58:14.311+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T18:58:14.311+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T18:58:14.313+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T18:58:14.313+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:58:14.321+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T18:58:15.327+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T18:58:18.912+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T18:58:23.038+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-18T18:58:23.043+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:16:58.217619+00:00, map_index=-1, run_start_date=2024-05-18 13:28:18.952268+00:00, run_end_date=2024-05-18 13:28:21.671382+00:00, run_duration=2.719114, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=46, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:28:14.312382+00:00, queued_by_job_id=36, pid=117815[0m
[[34m2024-05-18T19:01:45.151+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T19:03:11.350+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T19:03:11.351+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T19:03:11.351+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-18T19:03:11.352+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T19:03:11.352+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T19:03:11.362+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'scheduled__2024-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T19:03:12.366+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T19:03:15.964+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation scheduled__2024-05-05T00:00:00+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T19:03:20.328+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='scheduled__2024-05-05T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-05-18T19:03:20.332+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=scheduled__2024-05-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-18 13:33:16.009727+00:00, run_end_date=2024-05-18 13:33:19.006692+00:00, run_duration=2.996965, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=47, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:33:11.351678+00:00, queued_by_job_id=36, pid=118403[0m
[[34m2024-05-18T19:03:20.497+0530[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun sentimental_analysis_training_pipeline @ 2024-05-05 00:00:00+00:00: scheduled__2024-05-05T00:00:00+00:00, state:running, queued_at: 2024-05-18 13:16:58.828421+00:00. externally triggered: False> failed[0m
[[34m2024-05-18T19:03:20.498+0530[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=sentimental_analysis_training_pipeline, execution_date=2024-05-05 00:00:00+00:00, run_id=scheduled__2024-05-05T00:00:00+00:00, run_start_date=2024-05-18 13:16:58.878268+00:00, run_end_date=2024-05-18 13:33:20.498205+00:00, run_duration=981.619937, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-12 00:00:00+00:00, dag_hash=dfae8fa89ec57ec058f50008e44c7e8e[0m
[[34m2024-05-18T19:03:20.501+0530[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for sentimental_analysis_training_pipeline to 2024-05-12 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00[0m
[[34m2024-05-18T19:03:21.688+0530[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T19:03:21.688+0530[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG sentimental_analysis_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-05-18T19:03:21.688+0530[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [scheduled]>[0m
[[34m2024-05-18T19:03:21.689+0530[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-18T19:03:21.689+0530[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T19:03:21.698+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'sentimental_analysis_training_pipeline', 'data_transformation', 'manual__2024-05-18T13:16:58.217619+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-05-18T19:03:22.725+0530[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/skumar/DaatScience/AutonomizeAssign/airflow/dags/training_pipeline.py[0m
[[34m2024-05-18T19:03:26.352+0530[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: sentimental_analysis_training_pipeline.data_transformation manual__2024-05-18T13:16:58.217619+00:00 [queued]> on host skumar-pc[0m
/home/skumar/miniconda3/envs/autoassign/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[[34m2024-05-18T19:03:31.466+0530[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='sentimental_analysis_training_pipeline', task_id='data_transformation', run_id='manual__2024-05-18T13:16:58.217619+00:00', try_number=4, map_index=-1)[0m
[[34m2024-05-18T19:03:31.469+0530[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=sentimental_analysis_training_pipeline, task_id=data_transformation, run_id=manual__2024-05-18T13:16:58.217619+00:00, map_index=-1, run_start_date=2024-05-18 13:33:26.399103+00:00, run_end_date=2024-05-18 13:33:30.146484+00:00, run_duration=3.747381, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=48, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-18 13:33:21.689087+00:00, queued_by_job_id=36, pid=118442[0m
[[34m2024-05-18T19:03:31.628+0530[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun sentimental_analysis_training_pipeline @ 2024-05-18 13:16:58.217619+00:00: manual__2024-05-18T13:16:58.217619+00:00, state:running, queued_at: 2024-05-18 13:16:58.239834+00:00. externally triggered: True> failed[0m
[[34m2024-05-18T19:03:31.629+0530[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=sentimental_analysis_training_pipeline, execution_date=2024-05-18 13:16:58.217619+00:00, run_id=manual__2024-05-18T13:16:58.217619+00:00, run_start_date=2024-05-18 13:17:09.415415+00:00, run_end_date=2024-05-18 13:33:31.629207+00:00, run_duration=982.213792, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-12 00:00:00+00:00, dag_hash=dfae8fa89ec57ec058f50008e44c7e8e[0m
[[34m2024-05-18T19:06:14.571+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 19:06:14 +0530] [115552] [INFO] Handling signal: int
[2024-05-18 19:06:14 +0530] [115553] [INFO] Worker exiting (pid: 115553)
[2024-05-18 19:06:14 +0530] [115556] [INFO] Worker exiting (pid: 115556)
[2024-05-18 19:06:14 +0530] [115552] [INFO] Shutting down: Master
[[34m2024-05-18T19:06:15.587+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 115554. PIDs of all processes in the group: [115554][0m
[[34m2024-05-18T19:06:15.588+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 115554[0m
[[34m2024-05-18T19:06:15.760+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=115554, status='terminated', exitcode=0, started='18:46:44') (115554) terminated with exit code 0[0m
[[34m2024-05-18T19:06:15.780+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 115554. PIDs of all processes in the group: [][0m
[[34m2024-05-18T19:06:15.780+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 115554[0m
[[34m2024-05-18T19:06:15.781+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 115554 as process group is missing.[0m
[[34m2024-05-18T19:06:15.781+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T19:57:29.951+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T19:57:29.951+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 19:57:29 +0530] [126981] [INFO] Starting gunicorn 22.0.0
[2024-05-18 19:57:29 +0530] [126981] [INFO] Listening at: http://[::]:8793 (126981)
[2024-05-18 19:57:29 +0530] [126981] [INFO] Using worker: sync
[[34m2024-05-18T19:57:29.982+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T19:57:29.983+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 19:57:29 +0530] [126982] [INFO] Booting worker with pid: 126982
[[34m2024-05-18T19:57:29.988+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 126983[0m
[[34m2024-05-18T19:57:29.989+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T19:57:29.992+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T19:57:30.016+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 19:57:30 +0530] [126985] [INFO] Booting worker with pid: 126985
[[34m2024-05-18T19:57:49.892+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-05-18 19:57:49 +0530] [126981] [INFO] Handling signal: int
[2024-05-18 19:57:49 +0530] [126982] [INFO] Worker exiting (pid: 126982)
[2024-05-18 19:57:49 +0530] [126985] [INFO] Worker exiting (pid: 126985)
[2024-05-18 19:57:50 +0530] [126981] [INFO] Shutting down: Master
[[34m2024-05-18T19:57:50.907+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 126983. PIDs of all processes in the group: [126983][0m
[[34m2024-05-18T19:57:50.907+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 126983[0m
[[34m2024-05-18T19:57:51.079+0530[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=126983, status='terminated', exitcode=0, started='19:57:29') (126983) terminated with exit code 0[0m
[[34m2024-05-18T19:57:51.094+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 126983. PIDs of all processes in the group: [][0m
[[34m2024-05-18T19:57:51.094+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 126983[0m
[[34m2024-05-18T19:57:51.094+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 126983 as process group is missing.[0m
[[34m2024-05-18T19:57:51.094+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-05-18T19:57:54.388+0530[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-18T19:57:54.388+0530[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-05-18 19:57:54 +0530] [127375] [INFO] Starting gunicorn 22.0.0
[2024-05-18 19:57:54 +0530] [127375] [INFO] Listening at: http://[::]:8793 (127375)
[2024-05-18 19:57:54 +0530] [127375] [INFO] Using worker: sync
[[34m2024-05-18T19:57:54.420+0530[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-18T19:57:54.421+0530[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2024-05-18 19:57:54 +0530] [127376] [INFO] Booting worker with pid: 127376
[[34m2024-05-18T19:57:54.427+0530[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 127377[0m
[[34m2024-05-18T19:57:54.428+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T19:57:54.431+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-18T19:57:54.458+0530] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-05-18 19:57:54 +0530] [127378] [INFO] Booting worker with pid: 127378
[2024-05-18 19:58:45 +0530] [127375] [INFO] Handling signal: winch
[2024-05-18 19:59:43 +0530] [127375] [INFO] Handling signal: winch
[[34m2024-05-18T20:02:54.513+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T20:07:54.653+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 20:11:56 +0530] [127375] [INFO] Handling signal: winch
[2024-05-18 20:12:14 +0530] [127375] [INFO] Handling signal: winch
[2024-05-18 20:12:21 +0530] [127375] [INFO] Handling signal: winch
[2024-05-18 20:12:51 +0530] [127375] [INFO] Handling signal: winch
[[34m2024-05-18T20:12:54.793+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 20:17:11 +0530] [127375] [INFO] Handling signal: winch
[[34m2024-05-18T20:17:57.414+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T20:23:00.051+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T20:28:00.196+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 20:28:18 +0530] [127375] [INFO] Handling signal: winch
[2024-05-18 20:29:49 +0530] [127375] [INFO] Handling signal: winch
[[34m2024-05-18T20:33:02.433+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T20:38:02.575+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T20:43:02.721+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T20:48:02.866+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 20:52:52 +0530] [127375] [INFO] Handling signal: winch
[[34m2024-05-18T20:53:03.012+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T20:58:03.157+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T21:03:03.302+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T21:08:03.450+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 21:12:57 +0530] [127375] [INFO] Handling signal: winch
[[34m2024-05-18T21:13:03.596+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-18T21:18:05.815+0530[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2024-05-18 21:19:35 +0530] [127375] [INFO] Handling signal: hup
[2024-05-18 21:19:35 +0530] [127375] [INFO] Hang up: Master
[2024-05-18 21:19:35 +0530] [127375] [ERROR] Worker (pid:127376) was sent SIGHUP!
[2024-05-18 21:19:35 +0530] [144465] [INFO] Booting worker with pid: 144465
[2024-05-18 21:19:35 +0530] [127375] [ERROR] Worker (pid:127378) was sent SIGHUP!
[2024-05-18 21:19:35 +0530] [144472] [INFO] Booting worker with pid: 144472
[[34m2024-05-18T21:19:36.833+0530[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[2024-05-18 21:19:36 +0530] [127375] [INFO] Handling signal: term
[2024-05-18 21:19:36 +0530] [144472] [INFO] Worker exiting (pid: 144472)
[2024-05-18 21:19:36 +0530] [144465] [INFO] Worker exiting (pid: 144465)
[2024-05-18 21:19:36 +0530] [127375] [INFO] Shutting down: Master
[2024-05-18T21:19:36.997+0530] {process_utils.py:263} INFO - Waiting up to 5 seconds for processes to exit...
[[34m2024-05-18T21:19:37.034+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 127377. PIDs of all processes in the group: [][0m
[[34m2024-05-18T21:19:37.035+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 127377[0m
[[34m2024-05-18T21:19:37.035+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 127377 as process group is missing.[0m
[[34m2024-05-18T21:19:37.045+0530[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 127377. PIDs of all processes in the group: [][0m
[[34m2024-05-18T21:19:37.046+0530[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 127377[0m
[[34m2024-05-18T21:19:37.046+0530[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 127377 as process group is missing.[0m
[[34m2024-05-18T21:19:37.046+0530[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
